[{"content":"If you\u0026rsquo;ve ever tried to work with financial market data at scale, you know the problems:\nArbitrary resampling that hides the movements you actually care about. Your model says enter at $1,850.23, exit at $1,850.89—but your hourly candle shows High: $1,851.20, Low: $1,849.80. Did your stop-loss trigger first, or did you hit take-profit? The data doesn\u0026rsquo;t tell you.\nTools that break at scale. Libraries that crash on large downloads, can\u0026rsquo;t resume, take forever to decode, or try to load everything into memory. Half are abandoned, the other half weren\u0026rsquo;t built for production workloads.\nRate limits that turn a 3-day download into a 3-week ordeal. Dukascopy\u0026rsquo;s free datafeed is great—until you need 30 symbols across 20 years and you suddenly realize bottlenecked at 5 requests per second.\nExpensive APIs for data that should be free. Why pay hundreds per month when the raw data exists, if only there was a decent way to access it?\nOver the past few months, I downloaded and decoded 2,000,000+ hours of tick data from Dukascopy. Not because I enjoy infrastructure work—because I needed it for reinforcement learning research. So I built TickVault.\nThe Design I built TickVault around three principles:\nMirror Dukascopy\u0026rsquo;s structure 1:1. No reformatting, no \u0026ldquo;clever\u0026rdquo; reorganization. The filesystem layout matches the source URLs exactly. No surprises when you need to debug. Single source of truth.\nStore raw, compressed data. Keep the original .bi5 files as-is. If you change your resampling strategy in 6 months, you don\u0026rsquo;t re-download terabytes. If you need to reproduce results from a year ago, the data hasn\u0026rsquo;t been pre-processed into irrelevance. ELT over ETL—extract and store, transform when you need it.\nDecode on-demand. You want one day of EUR/USD? Decompress those 24 hourly chunks. You want 5 years of 30 symbols? Same code path, just more chunks. Memory usage stays constant because you\u0026rsquo;re not loading everything upfront.\nHere\u0026rsquo;s what that looks like in practice:\npip install tick-vault Download a month of gold tick data:\nfrom datetime import datetime from tick_vault import download_range await download_range( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) Read it back as a pandas DataFrame:\nfrom tick_vault import read_tick_data df = read_tick_data( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(df.head()) # time ask bid ask_volume bid_volume # 2024-01-01 00:00:01.234 2062.450 2062.430 150 230 # ... Why Existing Solutions Fall Short Why Tick Data? The Resampling Trap Most data sources give you pre-resampled data—hourly candles, 5-minute bars, daily OHLC. But who decided that\u0026rsquo;s the right granularity for your problem?\nAssumptions are the worse enemy of Machine Learning models. Let me explain more:\nMachine Learning models are \u0026ldquo;function approximators\u0026rdquo;. They will see what a function inputs, and what it outputs and try their best to mimic the behavior of that function that would results into the outcome. You wouldn\u0026rsquo;t expect a function to receive only a subset of the inputs it needs and still give you the same output, would you? So we need to give all of the required inputs to a model, then hope that model learns to output the correct label.\nTick data has some problems as it\u0026rsquo;s raw form:\nIt is too granular, this leads to huge dimensions and massive memory and compute requirements It is irregular and does not follow a structured pattern in time So, traditional approaches was to simply bin them, or \u0026ldquo;Resample\u0026rdquo; them. We would bucket the data into bins, and just handpicked some values. This was a more eye-pleasing and manageable approach for human interaction. This approach had several issues. Some top ones among them:\nWhat if a certain event happens so quickly that fits into one bucket? We used multiple bucket size representations also known as multi \u0026ldquo;timeframe charts\u0026rdquo; to try to mitigate that. What if there are some time cyclical pattern in data that because of our arbitrary choice of bin width(timeframe) we lose that view/insight? What features do we have to choose from each bucket? Traditionally OHLC or Open, High, Low, Close Resampled data breaks risk calculations in back-testing. Say your strategy generates an entry at $1,850.23 and an exit at $1,850.89—a tight 66-pip window. Your stop-loss is at $1,849.95. The next hourly candle shows [Open: $1,850.10, High: $1,851.20, Low: $1,849.80, Close: $1,850.95]. Did you hit your take-profit at $1,850.89? Or did the price drop to $1,849.95 first and stop you out? The candle doesn\u0026rsquo;t tell you. You can\u0026rsquo;t calculate your actual risk, can\u0026rsquo;t validate your strategy, can\u0026rsquo;t trust your backtest. Even if your entry and exit are far apart, sudden volatility can create the same problem. A single hourly candle that spans $10 might contain a flash crash that would have stopped you out—but the OHLC makes it look like smooth sailing. We had to make so many assumptions that we did not know if they are correct or not. If you\u0026rsquo;re training an ML model, why would you assume 1-hour bins are the optimal feature resolution? And even if hourly happens to work, why assume that Open, High, Low, Close captures what matters? You\u0026rsquo;re throwing away every tick between those four points and the order of events.\nIf you\u0026rsquo;re serious about modeling market dynamics, you need tick-level data. Everything else is a lossy approximation.\nTools That Don\u0026rsquo;t Scale So you decide you need tick data. After some search you figure out your best bet for high quality data is dukascopy. You search GitHub for \u0026ldquo;dukascopy python\u0026rdquo; and find\u0026hellip; a graveyard.\nHalf the repos haven\u0026rsquo;t been touched in 3+ years. Broken dependencies, no type hints, no tests. READMEs that promise but crash on Python 3.12. They worked once, for someone, in 2018.\nThe active ones weren\u0026rsquo;t built for production ML. You start downloading 2 years of EUR/USD. Six hours in, your connection drops. Start over from scratch—there\u0026rsquo;s no resume logic. Or manually read the logs, study the architecture and try to figure out what is missing, make a script and pray your script handles partial state.\nMemory management is an afterthought. Some libraries load entire datasets into RAM before processing. Fine for a weekend of data. Catastrophic when you need 10 years across 30 symbols.\nDecoding is painfully slow. Single-threaded, pure Python loops processing millions of ticks. Every inefficiency compounds when you\u0026rsquo;re working at scale. What should take minutes takes hours.\nI used to alo duct-tape together scripts for a while—custom resume logic, manual retry handling, homegrown decoders. Then I got tired of maintaining infrastructure when I should have been training models.\nThe Bandwidth Bottleneck Dukascopy\u0026rsquo;s datafeed is free and high-quality. It\u0026rsquo;s also aggressively rate-limited.\nYou can download maybe 5-10 requests per second before you start getting 429s ( errors) or 503s (service unavailable). That\u0026rsquo;s fine if you\u0026rsquo;re grabbing a week of one symbol. It\u0026rsquo;s a nightmare when you need:\n30 currency pairs 20 years of history each 24 hourly chunks per day = 5,256,000 individual requests At 5 requests/second with perfect uptime, that\u0026rsquo;s 12 days of continuous downloading. In practice, with retries, backoff delays, and occasional connection issues? Closer to 3 weeks.\nAnd that\u0026rsquo;s assuming you\u0026rsquo;re using a single connection. Most libraries don\u0026rsquo;t support concurrent downloads, let alone load distribution. You\u0026rsquo;re stuck babysitting a script for weeks, hoping nothing crashes overnight.\nThe bottleneck isn\u0026rsquo;t your internet connection. It\u0026rsquo;s the architecture of the tools.\nThe Cost vs Quality Tradeoff You could just pay for data. Plenty of vendors will sell you tick data—$X/month, enterprise contracts with minimums.\nFor a hedge fund, that\u0026rsquo;s a rounding error. For an independent researcher, a grad student, or someone in a country where $500/month is twice a salary? It\u0026rsquo;s a non-starter.\nAnd you\u0026rsquo;re often paying for convenience, not quality. Many paid APIs are just reselling the same Dukascopy data you could get for free, wrapped in a nicer interface. You\u0026rsquo;re paying someone else to solve the problem.\nWhich would be fine—if the free tools actually worked. But they don\u0026rsquo;t. So you\u0026rsquo;re stuck choosing between:\nExpensive APIs that solve the problem but price out independent research Free data that\u0026rsquo;s high-quality but inaccessible without building infrastructure Resampled garbage that\u0026rsquo;s easy to get but useless for serious work There should be a fourth option: free, high-quality, and actually usable at scale. That\u0026rsquo;s what TickVault is.\nHow TickVault Works Differently I didn\u0026rsquo;t want to build \u0026ldquo;yet another Dukascopy wrapper.\u0026rdquo; I wanted to solve the underlying architectural problems that make existing tools fragile. A minimal, pythonic, type-safe, performant, and scalable solution.\nStore Raw, Mirror 1:1 TickVault\u0026rsquo;s filesystem structure mirrors Dukascopy\u0026rsquo;s URL structure exactly:\ntick_vault_data/ └── downloads/ └── XAUUSD/ └── 2024/ └── 02/ # Month: 0-indexed (00=Jan, 11=Dec) └── 15/ # Day ├── 00h_ticks.bi5 ├── 01h_ticks.bi5 └── ... Every file is stored in its original compressed .bi5 format. No reformatting, no \u0026ldquo;clever\u0026rdquo; reorganization, no pre-processing.\nWhy this matters:\nSingle source of truth. When something breaks, you know exactly where to look. The file at XAUUSD/2024/02/15/14h_ticks.bi5 corresponds to https://datafeed.dukascopy.com/datafeed/XAUUSD/2024/02/15/14h_ticks.bi5. No mental mapping required.\nReproducibility. Your resampling strategy from 6 months ago produced different results today? The raw data hasn\u0026rsquo;t changed—you can investigate. With pre-processed data, you\u0026rsquo;re just guessing.\nStorage efficiency. Compressed tick data is surprisingly small. 20 years of Gold (200,000+ hourly files) is \u0026lt;15GB. Keep the originals, transform on-demand.\nFuture-proof. Decide you need millisecond timestamps instead of your current second-level precision? The raw ticks are still there. You\u0026rsquo;re not locked into past decisions.\nThis is ELT (Extract-Load-Transform) instead of ETL. Get the data once, transform it however many times you need.\nMetadata-Driven Resume Every download attempt gets tracked in a SQLite database:\nmetadata.db ├── symbol_XAUUSD │ ├── timestamp: 1704067200 (2024-01-01 00:00 UTC) │ ├── has_data: 1 │ └── ... └── symbol_EURUSD └── ... Each symbol gets its own table. Each hour gets a row with two pieces of information:\nDid we attempt to download it? Does data exist for this hour? (Some hours legitimately have no data—weekends for forex, market holidays, etc.) Why this matters:\nTrue resume capability. Your download crashes at hour 50,000? Run the same command again. TickVault checks the database, skips what\u0026rsquo;s already done, continues from where it left off. No manual state tracking.\nIncremental updates. Downloaded data through March? Now it\u0026rsquo;s June? Just set end=datetime.now() and TickVault only fetches the new chunks. Your historical data stays untouched.\nGap detection. Before reading data, TickVault verifies continuity. Missing hours in the middle of your range? It tells you before you waste time on a broken dataset.\nThe producer-consumer pattern: Download workers (producers) fetch chunks concurrently and push results to a queue. A single metadata worker (consumer) batches database writes. This avoids database lock contention while maintaining consistency—even with 500 parallel workers hammering away.\nParallel Everything TickVault\u0026rsquo;s download architecture is built around concurrency:\nOrchestrator ├── Proxy A → Worker 1, Worker 2, ..., Worker N ├── Proxy B → Worker 1, Worker 2, ..., Worker N └── Proxy C → Worker 1, Worker 2, ..., Worker N ↓ Result Queue ↓ Metadata Worker (single writer) Each proxy gets its own pool of async workers (default: 10 per proxy). Each worker:\nPulls a chunk from the work queue Fetches it via httpx with retry logic Saves the compressed data to disk Reports the result to the metadata worker Why this matters:\nSpeed. With 7 proxies and 10 workers each, you\u0026rsquo;re making 70 concurrent requests. That 12-day download? Now it\u0026rsquo;s done in hours.\nRate limit mitigation. Dukascopy rate-limits per IP. Distributing requests across proxies means you\u0026rsquo;re not constantly hitting limits and backing off.\nFault tolerance. One worker hangs? The other 69 keep going. One proxy gets blocked? Its workers fail gracefully while the others continue. The orchestrator handles backpressure—if downloads are faster than metadata writes, the queue fills up and workers naturally slow down.\nExponential backoff with context. Transient network error? Retry with increasing delays. Rate limit with Retry-After header? Respect it. Forbidden/blocked? Fail fast and stop wasting time.\nThe async architecture means you\u0026rsquo;re not waiting on I/O. While one worker is waiting for a response, 69 others are fetching, decoding, or writing.\nDecode On-Demand TickVault doesn\u0026rsquo;t pre-process anything. The .bi5 files stay compressed on disk until you actually need them.\nWhen you call read_tick_data():\nQuery the metadata database for available chunks in your time range Verify there are no gaps (fail fast if data is incomplete) Load each compressed chunk sequentially Decompress with LZMA, decode with NumPy structured arrays, fully vectorized Why this matters:\nFlexibility. Want to resample to 5-minute bars today and 1-second bars tomorrow? The raw ticks are still there. Want to calculate VWAP using actual volumes instead of approximations? You have the data. Every transformation is non-destructive.\nFast enough. Decompression and decoding are fast—LZMA is optimized, NumPy handles binary parsing efficiently. For most use cases, the bottleneck is your analysis code, not the data loading.\nComing soon: Incremental decode-to-database pipelines. Stream chunks directly to SQLite or HDF5 for efficient querying without loading into memory. Same raw source files, different materialization strategies—pick what fits your workflow. You can store N-TB of compressed data and work with 1GB at a time. The same code works whether you\u0026rsquo;re reading one day or ten years—memory usage stays constant because you\u0026rsquo;re streaming through chunks, not loading everything upfront.\nThe pattern is simple: download once, transform many times. Keep the highest-resolution version, derive everything else as needed.\nNow Some Code Basic Workflow \u0026ldquo;pip install tick-vault\u0026rdquo; if you haven\u0026rsquo;t already!\nDownload and read:\nfrom datetime import datetime from tick_vault import download_range, read_tick_data # Download a month of gold tick data await download_range( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) # Read it back as a pandas DataFrame df = read_tick_data( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(df.head()) # time ask bid ask_volume bid_volume # 2024-01-01 00:00:01.234 2062.450 2062.430 1500000 2300000 Speed It Up with Proxies # Use multiple proxies to distribute load and avoid rate limits await download_range( symbol=\u0026#39;EURUSD\u0026#39;, start=datetime(2000, 1, 1), end=None, # Or datetime.now() for the same effect proxies=[ \u0026#39;http://proxy1.example.com:8080\u0026#39;, \u0026#39;http://proxy2.example.com:8080\u0026#39;, \u0026#39;http://proxy3.example.com:8080\u0026#39; ] ) Configuration from tick_vault import reload_config # Customize settings reload_config( base_directory=\u0026#39;./my_tick_data\u0026#39;, worker_per_proxy=15, # More workers per proxy fetch_max_retry_attempts=5, # More retries for flaky connections metadata_update_batch_size=200 # Larger batches for efficiency ) Or use environment variables:\nexport TICK_VAULT_BASE_DIRECTORY=/data/ticks export TICK_VAULT_WORKER_PER_PROXY=15 export TICK_VAULT_CONSOLE_LOG_LEVEL=INFO Full configuration reference in the repo\nResuming and Incremental Updates # Download interrupted? Just run it again await download_range( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2020, 1, 1), end=datetime(2024, 1, 1) ) # TickVault checks metadata, skips completed chunks, resumes where it left off # Update with recent data await download_range( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 3, 1), end=datetime.now() ) # Only fetches the new chunks, leaves historical data untouched Data Validation # Strict mode (default): ensures the exact provided range is present df = read_tick_data( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1), strict=True # Raises error if any hours are missing ) # Non-strict mode: clips to available data range, still raises error if there are gaps between df = read_tick_data( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2020, 1, 1), # May be before first available end=datetime(2030, 1, 1), # May be after last available strict=False # Automatically adjusts to what exists ) Working with Metadata from tick_vault.metadata import MetadataDB with MetadataDB() as db: # Check what\u0026#39;s available first = db.first_chunk(\u0026#39;XAUUSD\u0026#39;) last = db.last_chunk(\u0026#39;XAUUSD\u0026#39;) print(f\u0026#34;Data range: {first.time} to {last.time}\u0026#34;) # Find what needs downloading pending = db.find_not_attempted_chunks( symbol=\u0026#39;EURUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(f\u0026#34;{len(pending)} chunks remaining\u0026#34;) # Verify continuity db.check_for_gaps( symbol=\u0026#39;XAUUSD\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) # Raises RuntimeError if gaps exist Custom Price Scales # For symbols not in the built-in registry df = read_tick_data( symbol=\u0026#39;CUSTOM_PAIR\u0026#39;, start=datetime(2024, 1, 1), end=datetime(2024, 2, 1), pipet_scale=0.01 # Specify your own scaling factor ) What\u0026rsquo;s Next TickVault works well for my needs, but there\u0026rsquo;s room to grow. Here\u0026rsquo;s what\u0026rsquo;s on the roadmap:\nDownload Pipeline:\nDynamic worker auto-balancing with throughput monitoring Adaptive scaling: gradually increase workers until throughput plateaus, then back off Async stop events for cleaner worker termination (currently uses sentinel values) Reading Pipeline:\nMulti-threading and multi-processing support for parallel decoding. Not generally needed since it is mostly I/O bound but for rapid SSDs might come handy! Streaming decode-to-SQLite pipeline for memory-efficient querying HDF5 storage backend option for large datasets Developer Experience:\nCLI interface for common operations (tickvault download XAUUSD --start 2024-01-01) Comprehensive pytest test suite General:\nReorganized module structure as the codebase grows Unified download_and_read() convenience function, so you don\u0026rsquo;t have to call two functions 🥱 More symbols added to the pipet scale registry The core is stable. These are refinements, not fundamental changes. If you have ideas or want to contribute, issues and PRs are welcome.\nTry It TickVault is open source on GitHub. If you\u0026rsquo;ve ever fought with financial data pipelines, give it a shot:\npip install tick-vault If it solves a problem for you, star the repo. Stars help other researchers find tools that actually work.\nIf you find issues or have ideas, open an issue or PR. The codebase is minimal and clean enough to understand in an afternoon.\nIf you just want to follow along, I\u0026rsquo;ll be writing more about the RL research this was built for, the performance optimizations, and lessons from building production data pipelines.\nI built TickVault because I needed tick data for reinforcement learning research and nothing else worked well enough.\nIf you\u0026rsquo;re in the same boat—tired of resampled data, broken tools, and expensive APIs—this might help.\nGitHub: keyhankamyar/TickVault\nLinkedIn: keyhan-kamyar\n","permalink":"https://keyhankamyar.github.io/posts/tickvault-introduction/","summary":"\u003cp\u003eIf you\u0026rsquo;ve ever tried to work with financial market data at scale, you know the problems:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eArbitrary resampling\u003c/strong\u003e that hides the movements you actually care about. Your model says enter at $1,850.23, exit at $1,850.89—but your hourly candle shows High: $1,851.20, Low: $1,849.80. Did your stop-loss trigger first, or did you hit take-profit? The data doesn\u0026rsquo;t tell you.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTools that break at scale\u003c/strong\u003e. Libraries that crash on large downloads, can\u0026rsquo;t resume, take forever to decode, or try to load everything into memory. Half are abandoned, the other half weren\u0026rsquo;t built for production workloads.\u003c/p\u003e","title":"I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To"},{"content":"👤 Background I\u0026rsquo;ve spent the last eight years building AI systems—not the kind that generate memes, but the kind that make decisions under uncertainty, process massive datasets, and run in production without falling over.\nStarted in urban engineering, quickly realized I needed more logic and less art. Switched to computer science. Built and scaled a retail business to fund my education, then left it to focus entirely on what actually kept me awake at night: making machines learn.\n🛤️ The Journey 2016-2017 • First Company — Co-founded MasterCom, tried game development, realized I was more interested in the systems than the games. Pivoted to UI development with Qt/QML. Shipped 13+ projects. Learned what I didn\u0026rsquo;t want to do.\n2017-2018 • Financial Markets — Discovered Forex trading. Six months of pure research before touching a demo account. Built backtesting systems, learned signal processing, feature engineering, risk management. Realized simple strategies don\u0026rsquo;t work—markets are adversarial.\n2018-2020 • Deep Learning — Dove into machine learning. Andrew Ng\u0026rsquo;s courses, then straight into PyTorch. Computer vision, NLP, time-series forecasting. Read papers as they dropped from arXiv. Built a dual RTX 2080 Ti rig. Implemented everything from scratch to understand how it actually works.\n2020-2021 • Dideo (Video Platform) — First real ML role. Built data pipelines processing petabytes of visual data. Custom Numba kernels that beat C++ implementations. Synthetic data generation systems. Learned production ML isn\u0026rsquo;t just about accuracy—it\u0026rsquo;s about throughput, maintainability, and human-in-the-loop workflows.\n2021-2022 • Deep RL — Deep dive into reinforcement learning. Traditional RL, then Deep RL. Built custom Gymnasium environments. Trained agents on financial markets. Spent six months failing to make stable models—learned more from failure than from any tutorial. Built scalable RL frameworks from scratch.\n2022-2024 • MetaScape (Fintech) — Built news scraping pipelines, sentiment analysis systems, portfolio optimization with RL. First experience with production CI/CD, structured logging, and the reality that 80% of ML engineering is data engineering.\n2024-Present • Independent Research — Reformulating trading as multi-armed bandits instead of sequential RL. Building systems that scale. Shipping open-source tools. Looking for the next challenge.\n💭 Technical Philosophy Theory is the map; implementation is the territory. I read papers to understand principles, then build systems to find out what actually works. Most breakthrough papers have implementation details that make or break their claims.\nOptimization isn\u0026rsquo;t premature if you understand the bottleneck. Custom Numba kernels, memory-mapped arrays, vectorized operations—these aren\u0026rsquo;t tricks, they\u0026rsquo;re prerequisites when you\u0026rsquo;re processing 100M+ rows.\nProduction systems need different skills than research code. Type safety with Pydantic, comprehensive logging, graceful degradation, monitoring, testing—these aren\u0026rsquo;t optional. They\u0026rsquo;re what separates a notebook from a product.\nThe best code is boring code. Clear names, simple logic, obvious structure. Clever code is a liability. Maintainable code is an asset.\n🎯 What I\u0026rsquo;m Looking For Research labs or companies working on:\nDeep RL for real-world control problems Financial ML / quantitative systems Large-scale ML infrastructure Systems that actually ship to users Places where \u0026ldquo;we should try that paper\u0026rdquo; leads to \u0026ldquo;here\u0026rsquo;s the implementation\u0026rdquo; within days, not months. Teams that value builders over talkers.\n📬 Contact ","permalink":"https://keyhankamyar.github.io/about/","summary":"\u003ch2 id=\"-background\"\u003e👤 Background\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve spent the last eight years building AI systems—not the kind that generate memes, but the kind that make decisions under uncertainty, process massive datasets, and run in production without falling over.\u003c/p\u003e\n\u003cp\u003eStarted in urban engineering, quickly realized I needed more logic and less art. Switched to computer science. Built and scaled a retail business to fund my education, then left it to focus entirely on what actually kept me awake at night: making machines learn.\u003c/p\u003e","title":"About"},{"content":"🔓 Open Source TickVault High-performance financial tick data pipeline for Dukascopy Bank\u0026rsquo;s historical datafeed. Built for quantitative researchers and algorithmic traders who need reliable access to high-resolution market data.\nFeatures:\nConcurrent downloads with intelligent resume capability Multi-Proxy pipeline for distributed downloading Efficient decompression and decoding SQLite metadata tracking and gap detection Pandas and NumPy integration Stack: Python • httpx • NumPy • Pandas • SQLite • LZMA\nProxyRotator An async Python library built for stealth for managing VMESS proxy and user-agent rotation with automatic subscription updates, connection testing, and automatic selection of most used user-agents for maximum anonymity. Perfect for web scraping projects requiring reliable proxy management.\nFeatures:\nAutomatic proxy rotation with subscription support Connection testing and filtering of working proxies User-agent rotation for each proxy Uses most used user-agents globally for maximum stealth Rate limiting with jitter for natural request patterns Thread-safe with context manager support Stack: Python • httpx • Xray-core • Pydantic • asyncio\n🔬 Coming Soon SAM2 Realtime Predictor Real-time video segmentation optimized for live camera feeds and streaming video. Text-prompted segmentation, custom kernel optimizations and TensorRT compilation.\nStatus: Polishing • Ready \u0026lt;2 weeks\nTunable Config Research-grade library unifying Optuna (HPO), neural architecture search, and feature selection with type-safe Pydantic configs. Distilled from years of experimentation into clean, reusable patterns.\nStatus: Planning • Expected in November\nClean-TS Modular, Pythonic reimplementation of canonical time-series architectures. Makes archaic, opaque TS codebases readable, extensible, and reproducible.\nStatus: Refactoring\n","permalink":"https://keyhankamyar.github.io/projects/","summary":"\u003ch2 id=\"-open-source\"\u003e🔓 Open Source\u003c/h2\u003e\n\u003ch3 id=\"tickvault\"\u003eTickVault\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/keyhankamyar/TickVault\"\u003e\u003cimg alt=\"GitHub\" loading=\"lazy\" src=\"https://img.shields.io/badge/GitHub-TickVault-181717?style=flat-square\u0026logo=github\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/tick-vault/\"\u003e\u003cimg alt=\"PyPI\" loading=\"lazy\" src=\"https://img.shields.io/pypi/v/tick-vault?style=flat-square\u0026logo=pypi\u0026logoColor=white\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.python.org/\"\u003e\u003cimg alt=\"Python\" loading=\"lazy\" src=\"https://img.shields.io/badge/Python-3.14-3776AB?style=flat-square\u0026logo=python\u0026logoColor=white\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eHigh-performance financial tick data pipeline for Dukascopy Bank\u0026rsquo;s historical datafeed. Built for quantitative researchers and algorithmic traders who need reliable access to high-resolution market data.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFeatures:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConcurrent downloads with intelligent resume capability\u003c/li\u003e\n\u003cli\u003eMulti-Proxy pipeline for distributed downloading\u003c/li\u003e\n\u003cli\u003eEfficient decompression and decoding\u003c/li\u003e\n\u003cli\u003eSQLite metadata tracking and gap detection\u003c/li\u003e\n\u003cli\u003ePandas and NumPy integration\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eStack:\u003c/strong\u003e Python • httpx • NumPy • Pandas • SQLite • LZMA\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"proxyrotator\"\u003eProxyRotator\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/keyhankamyar/ProxyRotator\"\u003e\u003cimg alt=\"GitHub\" loading=\"lazy\" src=\"https://img.shields.io/badge/GitHub-ProxyRotator-181717?style=flat-square\u0026logo=github\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/xray-proxy-rotator/\"\u003e\u003cimg alt=\"PyPI\" loading=\"lazy\" src=\"https://img.shields.io/pypi/v/xray-proxy-rotator?style=flat-square\u0026logo=pypi\u0026logoColor=white\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.python.org/\"\u003e\u003cimg alt=\"Python\" loading=\"lazy\" src=\"https://img.shields.io/badge/Python-3.14-3776AB?style=flat-square\u0026logo=python\u0026logoColor=white\"\u003e\u003c/a\u003e\u003c/p\u003e","title":"Projects"}]