<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To | Keyhan Kamyar</title><meta name=keywords content="financial tick data,tick data download,dukascopy python library,dukascopy tick data,forex tick data,historical market data,algorithmic trading data,quantitative finance tools,backtesting framework,high frequency trading data,reinforcement learning finance,machine learning trading,python trading library,market data pipeline,financial data engineering,tick data analysis,real-time market data,trading system development,quant research tools,open source trading tools"><meta name=description content="TickVault: Open-source Python library for downloading high-quality financial tick data from Dukascopy. Built for quantitative researchers and algorithmic traders. Handles parallel downloads, rate limits, resume capability, and on-demand decoding. Free alternative to expensive tick data APIs."><meta name=author content="Keyhan Kamyar"><link rel=canonical href=https://keyhankamyar.github.io/posts/tickvault-introduction/><link crossorigin=anonymous href=/assets/css/stylesheet.2e0be6fbe82e4f8483cbbaaefe8669dd6361daa06341b2c23c7fb904d6fb8610.css integrity="sha256-Lgvm++guT4SDy7qu/oZp3WNh2qBjQbLCPH+5BNb7hhA=" rel="preload stylesheet" as=style><link rel=icon href=https://keyhankamyar.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://keyhankamyar.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://keyhankamyar.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://keyhankamyar.github.io/apple-touch-icon.png><link rel=mask-icon href=https://keyhankamyar.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://keyhankamyar.github.io/posts/tickvault-introduction/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://keyhankamyar.github.io/posts/tickvault-introduction/"><meta property="og:site_name" content="Keyhan Kamyar"><meta property="og:title" content="I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To"><meta property="og:description" content="TickVault: Open-source Python library for downloading high-quality financial tick data from Dukascopy. Built for quantitative researchers and algorithmic traders. Handles parallel downloads, rate limits, resume capability, and on-demand decoding. Free alternative to expensive tick data APIs."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-10-15T12:00:00+03:30"><meta property="article:modified_time" content="2025-10-15T12:00:00+03:30"><meta property="article:tag" content="Machine-Learning"><meta property="article:tag" content="Finance"><meta property="article:tag" content="Open-Source"><meta property="article:tag" content="Python"><meta property="article:tag" content="Data-Engineering"><meta property="og:image" content="https://keyhankamyar.github.io/images/tickvault-og.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://keyhankamyar.github.io/images/tickvault-og.png"><meta name=twitter:title content="I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To"><meta name=twitter:description content="TickVault: Open-source Python library for downloading high-quality financial tick data from Dukascopy. Built for quantitative researchers and algorithmic traders. Handles parallel downloads, rate limits, resume capability, and on-demand decoding. Free alternative to expensive tick data APIs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://keyhankamyar.github.io/posts/"},{"@type":"ListItem","position":2,"name":"I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To","item":"https://keyhankamyar.github.io/posts/tickvault-introduction/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To","name":"I Downloaded 2,000,000\u002b Hours of Market Tick Data So You Don\u0027t Have To","description":"TickVault: Open-source Python library for downloading high-quality financial tick data from Dukascopy. Built for quantitative researchers and algorithmic traders. Handles parallel downloads, rate limits, resume capability, and on-demand decoding. Free alternative to expensive tick data APIs.","keywords":["financial tick data","tick data download","dukascopy python library","dukascopy tick data","forex tick data","historical market data","algorithmic trading data","quantitative finance tools","backtesting framework","high frequency trading data","reinforcement learning finance","machine learning trading","python trading library","market data pipeline","financial data engineering","tick data analysis","real-time market data","trading system development","quant research tools","open source trading tools"],"articleBody":"If youâ€™ve ever tried to work with financial market data at scale, you know the pain points:\nğŸ¯ The Core Problems Arbitrary resampling hides the movements you actually care about. Your model says enter at $1,850.23, exit at $1,850.89â€”but your hourly candle shows High: $1,851.20, Low: $1,849.80. Did your stop-loss trigger first, or did you hit take-profit? The data doesnâ€™t tell you.\nTools that break at scale. Libraries that crash on large downloads, canâ€™t resume, take forever to decode, or try to load everything into memory. Half are abandoned, the other half werenâ€™t built for production workloads.\nRate limits turn a 3-day download into a 3-week ordeal. Dukascopyâ€™s free datafeed is excellentâ€”until you need 30 symbols across 20 years and suddenly find yourself bottlenecked at 5 requests per second.\nExpensive APIs for data that should be free. Why pay hundreds per month when the raw data existsâ€”if only there was a proper way to access it?\nOver the past few months, I downloaded and decoded 2,000,000+ hours of tick data from Dukascopy. Not because I enjoy infrastructure workâ€”because I needed it for reinforcement learning research. So I built TickVault.\nğŸ—ï¸ The Design Philosophy I built TickVault around three core principles:\n1ï¸âƒ£ Mirror Dukascopyâ€™s Structure 1:1 No reformatting, no â€œcleverâ€ reorganization. The filesystem layout matches the source URLs exactly. No surprises when you need to debug. Single source of truth.\n2ï¸âƒ£ Store Raw, Compressed Data Keep the original .bi5 files as-is. Change your resampling strategy in 6 months? No re-downloading terabytes. Need to reproduce results from a year ago? The data hasnâ€™t been pre-processed into irrelevance.\nELT over ETL â€” extract and store, transform when you need it.\n3ï¸âƒ£ Decode On-Demand Want one day of EUR/USD? Decompress those 24 hourly chunks. Want 5 years of 30 symbols? Same code path, just more chunks. Memory usage stays constant because youâ€™re not loading everything upfront.\nğŸš€ Quick Start Hereâ€™s what that looks like in code:\npip install tick-vault Download a month of gold tick data:\nfrom datetime import datetime from tick_vault import download_range await download_range( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) Read it back as a pandas DataFrame:\nfrom tick_vault import read_tick_data df = read_tick_data( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(df.head()) # time ask bid ask_volume bid_volume # 2024-01-01 00:00:01.234 2062.450 2062.430 150 230 # ... âš ï¸ Why Existing Solutions Fall Short Why Tick Data? The Resampling Trap Most data sources give you pre-resampled dataâ€”hourly candles, 5-minute bars, daily OHLC. But who decided thatâ€™s the right granularity for your problem?\nAssumptions are the worst enemy of Machine Learning models. Hereâ€™s why:\nMachine Learning models are â€œfunction approximatorsâ€. They will see what a function inputs, and what it outputs and try their best to mimic the behavior of that function that would results into the outcome. You wouldnâ€™t expect a function to receive only a subset of the inputs it needs and still give you the same output, would you? So we need to give all of the required inputs to a model, then hope that model learns to output the correct label.\nRaw tick data has inherent challenges:\nToo granular â†’ Massive dimensions, heavy memory and compute requirements Irregular timing â†’ Doesnâ€™t follow structured patterns in time The traditional solution? Resampling. Bucket the data into fixed time bins and handpick representative values. More manageable for humans, but riddled with problems:\nâŒ Event Loss: Fast events compressed into one bucket lose critical detail. Multi-timeframe charts tried to help, but added complexity.\nâŒ Hidden Patterns: Arbitrary bin widths (1-hour, 5-minute) can mask cyclical patterns your model needs to see.\nâŒ Feature Limitation: OHLC (Open, High, Low, Close) throws away every tick between those four pointsâ€”and the order of events.\nâŒ Broken Backtesting: Your strategy enters at $1,850.23, exits at $1,850.89 (66-pip window). Stop-loss at $1,849.95. The hourly candle shows [O: 1850.10, H: 1851.20, L: 1849.80, C: 1850.95].\nDid you hit take-profit at $1,850.89? Or did price drop to $1,849.95 first and stop you out? The candle doesnâ€™t tell you. You canâ€™t calculate actual risk, canâ€™t validate your strategy, canâ€™t trust your backtest. A single candle spanning $10 might hide a flash crash that stopped you outâ€”but OHLC makes it look like smooth sailing.\nWe had to make so many assumptions that we did not know if they are correct or not. If youâ€™re training an ML model, why would you assume 1-hour bins are the optimal feature resolution? And even if hourly happens to work, why assume that Open, High, Low, Close captures what matters? Youâ€™re throwing away every tick between those four points and the order of events.\nIf youâ€™re serious about modeling market dynamics, you need tick-level data. Everything else is a lossy approximation.\nTools That Donâ€™t Scale You decide you need tick data. After searching, you find Dukascopy has high-quality data. You search GitHub for â€œdukascopy pythonâ€ and discoverâ€¦ a graveyard.\nThe Abandoned Half the repos havenâ€™t been touched in 3+ years. Broken dependencies, no type hints, no tests. READMEs that promise everything but crash on Python 3.12. They worked once, for someone, in 2018.\nThe Fragile The active ones werenâ€™t built for production ML. You start downloading 2 years of EUR/USD. Six hours in, your connection drops. Start over from scratchâ€”thereâ€™s no resume logic. Or manually parse logs, reverse-engineer the architecture, write recovery scripts, and pray they handle partial state correctly.\nThe Inefficient Memory management is an afterthought. Some libraries load entire datasets into RAM before processing. Fine for a weekend of data. Catastrophic when you need 10 years across 30 symbols.\nDecoding is painfully slow. Single-threaded, pure-Python loops processing millions of ticks. Every inefficiency compounds at scale. What should take minutes takes hours.\nI duct-taped together scripts for a whileâ€”custom resume logic, manual retry handling, homegrown decoders. Then I got tired of maintaining infrastructure when I should have been training models.\nThe Bandwidth Bottleneck Dukascopyâ€™s datafeed is free and high-quality. Itâ€™s also aggressively rate-limited.\nYou can download 5-10 requests/second before hitting 429 (rate limit) or 503 (service unavailable) errors. Fine for a week of one symbol. A nightmare when you need:\n30 currency pairs Ã— 20 years of history each Ã— 24 hourly chunks per day â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ = 5,256,000 individual requests At 5 requests/second with perfect uptime: 12 days of continuous downloading.\nIn practice, with retries, backoff delays, and connection issues? Closer to 3 weeks.\nAnd thatâ€™s with a single connection. Most libraries donâ€™t support concurrent downloads, let alone load distribution. Youâ€™re stuck babysitting a script for weeks, hoping nothing crashes overnight.\nThe bottleneck isnâ€™t your internet connection. Itâ€™s the architecture of the tools.\nThe Cost vs Quality Tradeoff You could just pay for data. Plenty of vendors sell tick dataâ€”$X/month, enterprise contracts with minimums.\nFor a hedge fund? A rounding error. For an independent researcher, grad student, or someone in a country where $500/month is twice a salary? A non-starter.\nAnd youâ€™re often paying for convenience, not quality. Many paid APIs just resell the same Dukascopy data you could get free, wrapped in a nicer interface. Youâ€™re paying someone else to solve the infrastructure problem.\nWhich would be fineâ€”if the free tools actually worked. But they donâ€™t.\nThe False Trilemma:\nğŸ’° Expensive APIs â€” Solve the problem but price out independent research ğŸ”“ Free data â€” High-quality but inaccessible without building infrastructure ğŸ—‘ï¸ Resampled data â€” Easy to get but useless for serious work There should be a fourth option: Free, high-quality, and actually usable at scale.\nThatâ€™s TickVault.\nâš™ï¸ How TickVault Works Differently I didnâ€™t want to build â€œyet another Dukascopy wrapper.â€ I wanted to solve the underlying architectural problems that make existing tools fragile.\nThe goal: Minimal, Pythonic, type-safe, performant, and scalable.\nğŸ—‚ï¸ Store Raw, Mirror 1:1 TickVaultâ€™s filesystem structure mirrors Dukascopyâ€™s URL structure exactly:\ntick_vault_data/ â””â”€â”€ downloads/ â””â”€â”€ XAUUSD/ â””â”€â”€ 2024/ â””â”€â”€ 02/ # Month: 0-indexed (00=Jan, 11=Dec) â””â”€â”€ 15/ # Day â”œâ”€â”€ 00h_ticks.bi5 â”œâ”€â”€ 01h_ticks.bi5 â””â”€â”€ ... Every file is stored in its original compressed .bi5 format. No reformatting, no â€œcleverâ€ reorganization, no pre-processing.\nWhy this matters:\nâœ… Single source of truth â€” When something breaks, you know exactly where to look. The file at XAUUSD/2024/02/15/14h_ticks.bi5 corresponds to https://datafeed.dukascopy.com/datafeed/XAUUSD/2024/02/15/14h_ticks.bi5. No mental mapping required.\nâœ… Reproducibility â€” Your resampling strategy from 6 months ago produced different results today? The raw data hasnâ€™t changedâ€”you can investigate. With pre-processed data, youâ€™re just guessing.\nâœ… Storage efficiency â€” Compressed tick data is surprisingly small. 20 years of Gold (200,000+ hourly files) is \u003c15GB. Keep the originals, transform on-demand.\nâœ… Future-proof â€” Need millisecond timestamps instead of second-level precision? The raw ticks are still there. Youâ€™re not locked into past decisions.\nELT over ETL â€” Extract-Load-Transform instead of Extract-Transform-Load. Get the data once, transform it however many times you need.\nMetadata-Driven Resume Every download attempt gets tracked in a SQLite database:\nmetadata.db â”œâ”€â”€ symbol_XAUUSD â”‚ â”œâ”€â”€ timestamp: 1704067200 (2024-01-01 00:00 UTC) â”‚ â”œâ”€â”€ has_data: 1 â”‚ â””â”€â”€ ... â””â”€â”€ symbol_EURUSD â””â”€â”€ ... Each symbol gets its own table. Each hour gets a row with two pieces of information:\nDid we attempt to download it? Does data exist for this hour? (Some hours legitimately have no dataâ€”weekends for forex, market holidays, etc.) Why this matters:\nâœ… True resume capability â€” Download crashes at hour 50,000? Run the same command again. TickVault checks the database, skips completed work, continues where it left off. Zero manual state tracking.\nâœ… Incremental updates â€” Downloaded through March? Now itâ€™s June? Set end=datetime.now() and TickVault only fetches new chunks. Historical data stays untouched.\nâœ… Gap detection â€” Before reading, TickVault verifies continuity. Missing hours in your range? It tells you before you waste time on a broken dataset.\nâœ… Producer-consumer pattern â€” Download workers (producers) fetch chunks concurrently and push to a queue. A single metadata worker (consumer) batches database writes. This avoids lock contention while maintaining consistencyâ€”even with 500 parallel workers.\nParallel Everything TickVaultâ€™s download architecture is built around concurrency:\nOrchestrator â”œâ”€â”€ Proxy A â†’ Worker 1, Worker 2, ..., Worker N â”œâ”€â”€ Proxy B â†’ Worker 1, Worker 2, ..., Worker N â””â”€â”€ Proxy C â†’ Worker 1, Worker 2, ..., Worker N â†“ Result Queue â†“ Metadata Worker (single writer) Each proxy gets its own pool of async workers (default: 10 per proxy). Each worker:\nPulls a chunk from the work queue Fetches it via httpx with retry logic Saves the compressed data to disk Reports the result to the metadata worker Why this matters:\nSpeed â€” With 7 proxies Ã— 10 workers = 70 concurrent requests. That 12-day download? Done in hours.\nRate limit mitigation â€” Dukascopy rate-limits per IP. Distributing across proxies means youâ€™re not constantly hitting limits and backing off.\nFault tolerance â€” One worker hangs? The other 69 keep going. One proxy blocked? Its workers fail gracefully while others continue. The orchestrator handles backpressureâ€”if downloads outpace metadata writes, the queue fills and workers naturally slow down.\nExponential backoff with context â€” Transient network error? Retry with increasing delays. Rate limit with Retry-After header? Respect it. Forbidden/blocked? Fail fast, stop wasting time.\nDecode On-Demand TickVault doesnâ€™t pre-process anything. The .bi5 files stay compressed on disk until you actually need them.\nWhen you call read_tick_data():\nQuery the metadata database for available chunks in your time range Verify there are no gaps (fail fast if data is incomplete) Load each compressed chunk sequentially Decompress with LZMA, decode with NumPy structured arrays, fully vectorized Why this matters:\nâœ… Flexibility â€” Resample to 5-minute bars today, 1-second bars tomorrow? The raw ticks are still there. Calculate VWAP using actual volumes? You have the data. Every transformation is non-destructive.\nâœ… Fast enough â€” Decompression and decoding are fastâ€”LZMA is optimized, NumPy handles binary parsing efficiently. For most use cases, the bottleneck is your analysis code, not the data loading.\nâœ… Coming soon â€” Incremental decode-to-database pipelines. Stream chunks directly to SQLite or HDF5 for efficient querying without loading into memory. Same raw source files, different materialization strategiesâ€”pick what fits your workflow. Store N-TB compressed, work with 1GB at a time. Same code for one day or ten yearsâ€”memory usage stays constant.\nThe pattern: Download once, transform many times. Keep the highest-resolution version, derive everything else as needed.\nğŸ‘£ Getting Started Install pip install tick-vault Basic Workflow Download and read tick data:\nfrom datetime import datetime from tick_vault import download_range, read_tick_data # Download a month of gold tick data await download_range( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) # Read it back as a pandas DataFrame df = read_tick_data( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(df.head()) # time ask bid ask_volume bid_volume # 2024-01-01 00:00:01.234 2062.450 2062.430 1500000 2300000 Speed It Up with Proxies # Use multiple proxies to distribute load and avoid rate limits await download_range( symbol='EURUSD', start=datetime(2000, 1, 1), end=None, # Or datetime.now() for the same effect proxies=[ 'http://proxy1.example.com:8080', 'http://proxy2.example.com:8080', 'http://proxy3.example.com:8080' ] ) Configuration from tick_vault import reload_config # Customize settings reload_config( base_directory='./my_tick_data', worker_per_proxy=15, # More workers per proxy fetch_max_retry_attempts=5, # More retries for flaky connections metadata_update_batch_size=200 # Larger batches for efficiency ) Or use environment variables:\nexport TICK_VAULT_BASE_DIRECTORY=/data/ticks export TICK_VAULT_WORKER_PER_PROXY=15 export TICK_VAULT_CONSOLE_LOG_LEVEL=INFO Full configuration reference in the repo\nResuming and Incremental Updates # Download interrupted? Just run it again await download_range( symbol='XAUUSD', start=datetime(2020, 1, 1), end=datetime(2024, 1, 1) ) # TickVault checks metadata, skips completed chunks, resumes where it left off # Update with recent data await download_range( symbol='XAUUSD', start=datetime(2024, 3, 1), end=datetime.now() ) # Only fetches the new chunks, leaves historical data untouched Data Validation # Strict mode (default): ensures the exact provided range is present df = read_tick_data( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1), strict=True # Raises error if any hours are missing ) # Non-strict mode: clips to available data range, still raises error if there are gaps between df = read_tick_data( symbol='XAUUSD', start=datetime(2020, 1, 1), # May be before first available end=datetime(2030, 1, 1), # May be after last available strict=False # Automatically adjusts to what exists ) Working with Metadata from tick_vault.metadata import MetadataDB with MetadataDB() as db: # Check what's available first = db.first_chunk('XAUUSD') last = db.last_chunk('XAUUSD') print(f\"Data range: {first.time} to {last.time}\") # Find what needs downloading pending = db.find_not_attempted_chunks( symbol='EURUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) print(f\"{len(pending)} chunks remaining\") # Verify continuity db.check_for_gaps( symbol='XAUUSD', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1) ) # Raises RuntimeError if gaps exist Custom Price Scales # For symbols not in the built-in registry df = read_tick_data( symbol='CUSTOM_PAIR', start=datetime(2024, 1, 1), end=datetime(2024, 2, 1), pipet_scale=0.01 # Specify your own scaling factor ) ğŸ§­ Whatâ€™s Next TickVault works well for my needs, but thereâ€™s room to grow. Hereâ€™s the roadmap:\nDownload Pipeline Dynamic worker auto-balancing with throughput monitoring Adaptive scaling: gradually increase workers until throughput plateaus, then back off Async stop events for cleaner worker termination (currently uses sentinel values) Reading Pipeline Multi-threading and multi-processing support for parallel decoding (helpful for rapid SSDs) Streaming decode-to-SQLite pipeline for memory-efficient querying HDF5 storage backend option for large datasets Developer Experience CLI interface for common operations (tickvault download XAUUSD --start 2024-01-01) Comprehensive pytest test suite General Improvements Reorganized module structure as the codebase grows Unified download_and_read() convenience function More symbols added to the pipet scale registry The core is stable. These are refinements, not fundamental changes. If you have ideas or want to contribute, issues and PRs are welcome.\nâ–¶ï¸ Try It Out TickVault is open source on GitHub. If youâ€™ve fought with financial data pipelines, give it a shot:\npip install tick-vault Star the repo if it solves a problem for you â€” Stars help other researchers discover tools that actually work.\nOpen an issue or PR if you find bugs or have ideas â€” The codebase is minimal and clean enough to understand in an afternoon.\nFollow along for more posts on the RL research this was built for, performance optimizations, and lessons from building production data pipelines.\nğŸ’­ Final Thoughts I built TickVault because I needed tick data for reinforcement learning research and nothing else worked well enough.\nIf youâ€™re in the same boatâ€”tired of resampled data, broken tools, and expensive APIsâ€”this might help.\nConnect:\n","wordCount":"2672","inLanguage":"en","image":"https://keyhankamyar.github.io/images/tickvault-og.png","datePublished":"2025-10-15T12:00:00+03:30","dateModified":"2025-10-15T12:00:00+03:30","author":{"@type":"Person","name":"Keyhan Kamyar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://keyhankamyar.github.io/posts/tickvault-introduction/"},"publisher":{"@type":"Person","name":"Keyhan Kamyar","logo":{"@type":"ImageObject","url":"https://keyhankamyar.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://keyhankamyar.github.io/ accesskey=h title="Keyhan Kamyar (Alt + H)">Keyhan Kamyar</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://keyhankamyar.github.io/about/ title=About><span>About</span></a></li><li><a href=https://keyhankamyar.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://keyhankamyar.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://keyhankamyar.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://keyhankamyar.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://keyhankamyar.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To</h1><div class=post-description>TickVault: Open-source Python library for downloading high-quality financial tick data from Dukascopy. Built for quantitative researchers and algorithmic traders. Handles parallel downloads, rate limits, resume capability, and on-demand decoding. Free alternative to expensive tick data APIs.</div><div class=post-meta><span title='2025-10-15 12:00:00 +0330 +0330'>October 15, 2025</span>&nbsp;Â·&nbsp;<span>13 min</span>&nbsp;Â·&nbsp;<span>2672 words</span>&nbsp;Â·&nbsp;<span>Keyhan Kamyar</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#-the-core-problems>ğŸ¯ The Core Problems</a></li></ul></li><li><a href=#-the-design-philosophy>ğŸ—ï¸ The Design Philosophy</a><ul><li><a href=#1-mirror-dukascopys-structure-11>1ï¸âƒ£ Mirror Dukascopy&rsquo;s Structure 1:1</a></li><li><a href=#2-store-raw-compressed-data>2ï¸âƒ£ Store Raw, Compressed Data</a></li><li><a href=#3-decode-on-demand>3ï¸âƒ£ Decode On-Demand</a></li><li><a href=#-quick-start>ğŸš€ Quick Start</a></li></ul></li><li><a href=#-why-existing-solutions-fall-short>âš ï¸ Why Existing Solutions Fall Short</a><ul><li><a href=#why-tick-data-the-resampling-trap>Why Tick Data? The Resampling Trap</a></li><li><a href=#tools-that-dont-scale>Tools That Don&rsquo;t Scale</a></li><li><a href=#the-bandwidth-bottleneck>The Bandwidth Bottleneck</a></li><li><a href=#the-cost-vs-quality-tradeoff>The Cost vs Quality Tradeoff</a></li></ul></li><li><a href=#-how-tickvault-works-differently>âš™ï¸ How TickVault Works Differently</a><ul><li><a href=#-store-raw-mirror-11>ğŸ—‚ï¸ Store Raw, Mirror 1:1</a></li><li><a href=#metadata-driven-resume>Metadata-Driven Resume</a></li><li><a href=#parallel-everything>Parallel Everything</a></li><li><a href=#decode-on-demand>Decode On-Demand</a></li></ul></li><li><a href=#-getting-started>ğŸ‘£ Getting Started</a><ul><li><a href=#install>Install</a></li><li><a href=#basic-workflow>Basic Workflow</a></li><li><a href=#speed-it-up-with-proxies>Speed It Up with Proxies</a></li><li><a href=#configuration>Configuration</a></li><li><a href=#resuming-and-incremental-updates>Resuming and Incremental Updates</a></li><li><a href=#data-validation>Data Validation</a></li><li><a href=#working-with-metadata>Working with Metadata</a></li><li><a href=#custom-price-scales>Custom Price Scales</a></li></ul></li><li><a href=#-whats-next>ğŸ§­ What&rsquo;s Next</a><ul><li><a href=#download-pipeline>Download Pipeline</a></li><li><a href=#reading-pipeline>Reading Pipeline</a></li><li><a href=#developer-experience>Developer Experience</a></li><li><a href=#general-improvements>General Improvements</a></li></ul></li><li><a href=#-try-it-out>â–¶ï¸ Try It Out</a></li><li><a href=#-final-thoughts>ğŸ’­ Final Thoughts</a></li></ul></nav></div></details></div><div class=post-content><p>If you&rsquo;ve ever tried to work with financial market data at scale, you know the pain points:</p><hr><h3 id=-the-core-problems>ğŸ¯ The Core Problems<a hidden class=anchor aria-hidden=true href=#-the-core-problems>#</a></h3><p><strong>Arbitrary resampling</strong> hides the movements you actually care about. Your model says enter at $1,850.23, exit at $1,850.89â€”but your hourly candle shows High: $1,851.20, Low: $1,849.80. Did your stop-loss trigger first, or did you hit take-profit? <em>The data doesn&rsquo;t tell you.</em></p><p><strong>Tools that break at scale.</strong> Libraries that crash on large downloads, can&rsquo;t resume, take forever to decode, or try to load everything into memory. Half are abandoned, the other half weren&rsquo;t built for production workloads.</p><p><strong>Rate limits</strong> turn a 3-day download into a 3-week ordeal. Dukascopy&rsquo;s free datafeed is excellentâ€”until you need 30 symbols across 20 years and suddenly find yourself bottlenecked at 5 requests per second.</p><p><strong>Expensive APIs</strong> for data that should be free. Why pay hundreds per month when the raw data existsâ€”if only there was a proper way to access it?</p><blockquote><p>Over the past few months, I downloaded and decoded <strong>2,000,000+ hours</strong> of tick data from Dukascopy. Not because I enjoy infrastructure workâ€”because I needed it for reinforcement learning research. So I built <strong>TickVault</strong>.</p></blockquote><hr><h2 id=-the-design-philosophy>ğŸ—ï¸ The Design Philosophy<a hidden class=anchor aria-hidden=true href=#-the-design-philosophy>#</a></h2><p>I built TickVault around three core principles:</p><h3 id=1-mirror-dukascopys-structure-11>1ï¸âƒ£ Mirror Dukascopy&rsquo;s Structure 1:1<a hidden class=anchor aria-hidden=true href=#1-mirror-dukascopys-structure-11>#</a></h3><p>No reformatting, no &ldquo;clever&rdquo; reorganization. The filesystem layout matches the source URLs exactly. No surprises when you need to debug. <strong>Single source of truth.</strong></p><h3 id=2-store-raw-compressed-data>2ï¸âƒ£ Store Raw, Compressed Data<a hidden class=anchor aria-hidden=true href=#2-store-raw-compressed-data>#</a></h3><p>Keep the original <code>.bi5</code> files as-is. Change your resampling strategy in 6 months? No re-downloading terabytes. Need to reproduce results from a year ago? The data hasn&rsquo;t been pre-processed into irrelevance.</p><p><strong>ELT over ETL</strong> â€” extract and store, transform when you need it.</p><h3 id=3-decode-on-demand>3ï¸âƒ£ Decode On-Demand<a hidden class=anchor aria-hidden=true href=#3-decode-on-demand>#</a></h3><p>Want one day of EUR/USD? Decompress those 24 hourly chunks. Want 5 years of 30 symbols? Same code path, just more chunks. Memory usage stays constant because you&rsquo;re not loading everything upfront.</p><hr><h3 id=-quick-start>ğŸš€ Quick Start<a hidden class=anchor aria-hidden=true href=#-quick-start>#</a></h3><p>Here&rsquo;s what that looks like in code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install tick-vault
</span></span></code></pre></div><p>Download a month of gold tick data:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tick_vault</span> <span class=kn>import</span> <span class=n>download_range</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>download_range</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Read it back as a pandas DataFrame:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tick_vault</span> <span class=kn>import</span> <span class=n>read_tick_data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>read_tick_data</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=c1># time                     ask        bid      ask_volume  bid_volume</span>
</span></span><span class=line><span class=cl><span class=c1># 2024-01-01 00:00:01.234  2062.450  2062.430  150         230</span>
</span></span><span class=line><span class=cl><span class=c1># ...</span>
</span></span></code></pre></div><hr><h2 id=-why-existing-solutions-fall-short>âš ï¸ Why Existing Solutions Fall Short<a hidden class=anchor aria-hidden=true href=#-why-existing-solutions-fall-short>#</a></h2><h3 id=why-tick-data-the-resampling-trap>Why Tick Data? The Resampling Trap<a hidden class=anchor aria-hidden=true href=#why-tick-data-the-resampling-trap>#</a></h3><p>Most data sources give you pre-resampled dataâ€”hourly candles, 5-minute bars, daily OHLC. But who decided that&rsquo;s the right granularity for <em>your</em> problem?</p><p><strong>Assumptions are the worst enemy of Machine Learning models.</strong> Here&rsquo;s why:</p><p>Machine Learning models are <strong>&ldquo;function approximators&rdquo;</strong>. They will see what a function inputs, and what it outputs and try their best to mimic the behavior of that function that would results into the outcome. You wouldn&rsquo;t expect a function to receive only a subset of the inputs it needs and still give you the same output, would you? So we need to give all of the required inputs to a model, then hope that model learns to output the correct label.</p><p>Raw tick data has inherent challenges:</p><ul><li><strong>Too granular</strong> â†’ Massive dimensions, heavy memory and compute requirements</li><li><strong>Irregular timing</strong> â†’ Doesn&rsquo;t follow structured patterns in time</li></ul><p>The traditional solution? <strong>Resampling.</strong> Bucket the data into fixed time bins and handpick representative values. More manageable for humans, but riddled with problems:</p><p><strong>âŒ Event Loss:</strong> Fast events compressed into one bucket lose critical detail. Multi-timeframe charts tried to help, but added complexity.</p><p><strong>âŒ Hidden Patterns:</strong> Arbitrary bin widths (1-hour, 5-minute) can mask cyclical patterns your model needs to see.</p><p><strong>âŒ Feature Limitation:</strong> OHLC (Open, High, Low, Close) throws away every tick between those four pointsâ€”and the <em>order</em> of events.</p><p><strong>âŒ Broken Backtesting:</strong> Your strategy enters at $1,850.23, exits at $1,850.89 (66-pip window). Stop-loss at $1,849.95. The hourly candle shows <code>[O: 1850.10, H: 1851.20, L: 1849.80, C: 1850.95]</code>.</p><p>Did you hit take-profit at $1,850.89? Or did price drop to $1,849.95 first and stop you out? <strong>The candle doesn&rsquo;t tell you.</strong> You can&rsquo;t calculate actual risk, can&rsquo;t validate your strategy, can&rsquo;t trust your backtest. A single candle spanning $10 might hide a flash crash that stopped you outâ€”but OHLC makes it look like smooth sailing.</p><p>We had to make so many assumptions that we did not know if they are correct or not. If you&rsquo;re training an ML model, why would you assume 1-hour bins are the optimal feature resolution? And even if hourly happens to work, why assume that Open, High, Low, Close captures what matters? You&rsquo;re throwing away every tick between those four points and the order of events.</p><p>If you&rsquo;re serious about modeling market dynamics, you need tick-level data. Everything else is a lossy approximation.</p><h3 id=tools-that-dont-scale>Tools That Don&rsquo;t Scale<a hidden class=anchor aria-hidden=true href=#tools-that-dont-scale>#</a></h3><p>You decide you need tick data. After searching, you find Dukascopy has high-quality data. You search GitHub for &ldquo;dukascopy python&rdquo; and discover&mldr; <strong>a graveyard.</strong></p><h4 id=the-abandoned>The Abandoned<a hidden class=anchor aria-hidden=true href=#the-abandoned>#</a></h4><p><strong>Half the repos haven&rsquo;t been touched in 3+ years.</strong> Broken dependencies, no type hints, no tests. READMEs that promise everything but crash on Python 3.12. They worked once, for someone, in 2018.</p><h4 id=the-fragile>The Fragile<a hidden class=anchor aria-hidden=true href=#the-fragile>#</a></h4><p><strong>The active ones weren&rsquo;t built for production ML.</strong> You start downloading 2 years of EUR/USD. Six hours in, your connection drops. Start over from scratchâ€”there&rsquo;s no resume logic. Or manually parse logs, reverse-engineer the architecture, write recovery scripts, and <em>pray</em> they handle partial state correctly.</p><h4 id=the-inefficient>The Inefficient<a hidden class=anchor aria-hidden=true href=#the-inefficient>#</a></h4><p><strong>Memory management is an afterthought.</strong> Some libraries load entire datasets into RAM before processing. Fine for a weekend of data. Catastrophic when you need 10 years across 30 symbols.</p><p><strong>Decoding is painfully slow.</strong> Single-threaded, pure-Python loops processing millions of ticks. Every inefficiency compounds at scale. What should take minutes takes hours.</p><blockquote><p>I duct-taped together scripts for a whileâ€”custom resume logic, manual retry handling, homegrown decoders. Then I got tired of maintaining infrastructure when I should have been training models.</p></blockquote><h3 id=the-bandwidth-bottleneck>The Bandwidth Bottleneck<a hidden class=anchor aria-hidden=true href=#the-bandwidth-bottleneck>#</a></h3><p>Dukascopy&rsquo;s datafeed is free and high-quality. It&rsquo;s also <strong>aggressively rate-limited.</strong></p><p>You can download 5-10 requests/second before hitting <code>429</code> (rate limit) or <code>503</code> (service unavailable) errors. Fine for a week of one symbol. A nightmare when you need:</p><pre tabindex=0><code>30 currency pairs
Ã— 20 years of history each  
Ã— 24 hourly chunks per day
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
= 5,256,000 individual requests
</code></pre><p>At 5 requests/second with perfect uptime: <strong>12 days of continuous downloading.</strong></p><p>In practice, with retries, backoff delays, and connection issues? <strong>Closer to 3 weeks.</strong></p><p>And that&rsquo;s with a <em>single</em> connection. Most libraries don&rsquo;t support concurrent downloads, let alone load distribution. You&rsquo;re stuck babysitting a script for weeks, hoping nothing crashes overnight.</p><blockquote><p><strong>The bottleneck isn&rsquo;t your internet connection. It&rsquo;s the architecture of the tools.</strong></p></blockquote><h3 id=the-cost-vs-quality-tradeoff>The Cost vs Quality Tradeoff<a hidden class=anchor aria-hidden=true href=#the-cost-vs-quality-tradeoff>#</a></h3><p>You could just <em>pay</em> for data. Plenty of vendors sell tick dataâ€”$X/month, enterprise contracts with minimums.</p><p>For a hedge fund? A rounding error. For an independent researcher, grad student, or someone in a country where <strong>$500/month is twice a salary</strong>? A non-starter.</p><p>And you&rsquo;re often paying for <strong>convenience, not quality.</strong> Many paid APIs just resell the same Dukascopy data you could get free, wrapped in a nicer interface. You&rsquo;re paying someone else to solve the infrastructure problem.</p><p>Which would be fineâ€”if the free tools actually worked. But they don&rsquo;t.</p><p><strong>The False Trilemma:</strong></p><ol><li>ğŸ’° <strong>Expensive APIs</strong> â€” Solve the problem but price out independent research</li><li>ğŸ”“ <strong>Free data</strong> â€” High-quality but inaccessible without building infrastructure</li><li>ğŸ—‘ï¸ <strong>Resampled data</strong> â€” Easy to get but useless for serious work</li></ol><blockquote><p><strong>There should be a fourth option:</strong> Free, high-quality, and actually usable at scale.</p></blockquote><blockquote><p><strong>That&rsquo;s TickVault.</strong></p></blockquote><hr><h2 id=-how-tickvault-works-differently>âš™ï¸ How TickVault Works Differently<a hidden class=anchor aria-hidden=true href=#-how-tickvault-works-differently>#</a></h2><p>I didn&rsquo;t want to build &ldquo;yet another Dukascopy wrapper.&rdquo; I wanted to solve the <strong>underlying architectural problems</strong> that make existing tools fragile.</p><p><strong>The goal:</strong> Minimal, Pythonic, type-safe, performant, and scalable.</p><h3 id=-store-raw-mirror-11>ğŸ—‚ï¸ Store Raw, Mirror 1:1<a hidden class=anchor aria-hidden=true href=#-store-raw-mirror-11>#</a></h3><p>TickVault&rsquo;s filesystem structure mirrors Dukascopy&rsquo;s URL structure <strong>exactly:</strong></p><pre tabindex=0><code>tick_vault_data/
â””â”€â”€ downloads/
    â””â”€â”€ XAUUSD/
        â””â”€â”€ 2024/
            â””â”€â”€ 02/          # Month: 0-indexed (00=Jan, 11=Dec)
                â””â”€â”€ 15/      # Day
                    â”œâ”€â”€ 00h_ticks.bi5
                    â”œâ”€â”€ 01h_ticks.bi5
                    â””â”€â”€ ...
</code></pre><p>Every file is stored in its original compressed <code>.bi5</code> format. No reformatting, no &ldquo;clever&rdquo; reorganization, no pre-processing.</p><p><strong>Why this matters:</strong></p><p>âœ… <strong>Single source of truth</strong> â€” When something breaks, you know exactly where to look. The file at <code>XAUUSD/2024/02/15/14h_ticks.bi5</code> corresponds to <code>https://datafeed.dukascopy.com/datafeed/XAUUSD/2024/02/15/14h_ticks.bi5</code>. No mental mapping required.</p><p>âœ… <strong>Reproducibility</strong> â€” Your resampling strategy from 6 months ago produced different results today? The raw data hasn&rsquo;t changedâ€”you can investigate. With pre-processed data, you&rsquo;re just guessing.</p><p>âœ… <strong>Storage efficiency</strong> â€” Compressed tick data is surprisingly small. 20 years of Gold (200,000+ hourly files) is <strong>&lt;15GB</strong>. Keep the originals, transform on-demand.</p><p>âœ… <strong>Future-proof</strong> â€” Need millisecond timestamps instead of second-level precision? The raw ticks are still there. You&rsquo;re not locked into past decisions.</p><blockquote><p><strong>ELT over ETL</strong> â€” Extract-Load-Transform instead of Extract-Transform-Load. Get the data once, transform it however many times you need.</p></blockquote><h3 id=metadata-driven-resume>Metadata-Driven Resume<a hidden class=anchor aria-hidden=true href=#metadata-driven-resume>#</a></h3><p>Every download attempt gets tracked in a SQLite database:</p><pre tabindex=0><code>metadata.db
â”œâ”€â”€ symbol_XAUUSD
â”‚   â”œâ”€â”€ timestamp: 1704067200  (2024-01-01 00:00 UTC)
â”‚   â”œâ”€â”€ has_data: 1
â”‚   â””â”€â”€ ...
â””â”€â”€ symbol_EURUSD
    â””â”€â”€ ...
</code></pre><p>Each symbol gets its own table. Each hour gets a row with two pieces of information:</p><ol><li>Did we attempt to download it?</li><li>Does data exist for this hour? (Some hours legitimately have no dataâ€”weekends for forex, market holidays, etc.)</li></ol><p><strong>Why this matters:</strong></p><p>âœ… <strong>True resume capability</strong> â€” Download crashes at hour 50,000? Run the same command again. TickVault checks the database, skips completed work, continues where it left off. Zero manual state tracking.</p><p>âœ… <strong>Incremental updates</strong> â€” Downloaded through March? Now it&rsquo;s June? Set <code>end=datetime.now()</code> and TickVault only fetches new chunks. Historical data stays untouched.</p><p>âœ… <strong>Gap detection</strong> â€” Before reading, TickVault verifies continuity. Missing hours in your range? It tells you <em>before</em> you waste time on a broken dataset.</p><p>âœ… <strong>Producer-consumer pattern</strong> â€” Download workers (producers) fetch chunks concurrently and push to a queue. A single metadata worker (consumer) batches database writes. This avoids lock contention while maintaining consistencyâ€”even with 500 parallel workers.</p><h3 id=parallel-everything>Parallel Everything<a hidden class=anchor aria-hidden=true href=#parallel-everything>#</a></h3><p>TickVault&rsquo;s download architecture is built around concurrency:</p><pre tabindex=0><code>Orchestrator
â”œâ”€â”€ Proxy A â†’ Worker 1, Worker 2, ..., Worker N
â”œâ”€â”€ Proxy B â†’ Worker 1, Worker 2, ..., Worker N
â””â”€â”€ Proxy C â†’ Worker 1, Worker 2, ..., Worker N
     â†“
  Result Queue
     â†“
  Metadata Worker (single writer)
</code></pre><p>Each proxy gets its own pool of async workers (default: 10 per proxy). Each worker:</p><ol><li>Pulls a chunk from the work queue</li><li>Fetches it via <code>httpx</code> with retry logic</li><li>Saves the compressed data to disk</li><li>Reports the result to the metadata worker</li></ol><p><strong>Why this matters:</strong></p><p><strong>Speed</strong> â€” With 7 proxies Ã— 10 workers = 70 concurrent requests. That 12-day download? <strong>Done in hours.</strong></p><p><strong>Rate limit mitigation</strong> â€” Dukascopy rate-limits per IP. Distributing across proxies means you&rsquo;re not constantly hitting limits and backing off.</p><p><strong>Fault tolerance</strong> â€” One worker hangs? The other 69 keep going. One proxy blocked? Its workers fail gracefully while others continue. The orchestrator handles backpressureâ€”if downloads outpace metadata writes, the queue fills and workers naturally slow down.</p><p><strong>Exponential backoff with context</strong> â€” Transient network error? Retry with increasing delays. Rate limit with <code>Retry-After</code> header? Respect it. Forbidden/blocked? Fail fast, stop wasting time.</p><h3 id=decode-on-demand>Decode On-Demand<a hidden class=anchor aria-hidden=true href=#decode-on-demand>#</a></h3><p>TickVault doesn&rsquo;t pre-process anything. The <code>.bi5</code> files stay compressed on disk until you actually need them.</p><p><strong>When you call <code>read_tick_data()</code>:</strong></p><ol><li>Query the metadata database for available chunks in your time range</li><li>Verify there are no gaps (fail fast if data is incomplete)</li><li>Load each compressed chunk sequentially</li><li>Decompress with LZMA, decode with NumPy structured arrays, fully vectorized</li></ol><p><strong>Why this matters:</strong></p><p>âœ… <strong>Flexibility</strong> â€” Resample to 5-minute bars today, 1-second bars tomorrow? The raw ticks are still there. Calculate VWAP using actual volumes? You have the data. Every transformation is non-destructive.</p><p>âœ… <strong>Fast enough</strong> â€” Decompression and decoding are fastâ€”LZMA is optimized, NumPy handles binary parsing efficiently. For most use cases, the bottleneck is your analysis code, not the data loading.</p><p>âœ… <strong>Coming soon</strong> â€” Incremental decode-to-database pipelines. Stream chunks directly to SQLite or HDF5 for efficient querying without loading into memory. Same raw source files, different materialization strategiesâ€”pick what fits your workflow. Store N-TB compressed, work with 1GB at a time. Same code for one day or ten yearsâ€”memory usage stays constant.</p><blockquote><p><strong>The pattern:</strong> Download once, transform many times. Keep the highest-resolution version, derive everything else as needed.</p></blockquote><hr><h2 id=-getting-started>ğŸ‘£ Getting Started<a hidden class=anchor aria-hidden=true href=#-getting-started>#</a></h2><h3 id=install>Install<a hidden class=anchor aria-hidden=true href=#install>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install tick-vault
</span></span></code></pre></div><h3 id=basic-workflow>Basic Workflow<a hidden class=anchor aria-hidden=true href=#basic-workflow>#</a></h3><p>Download and read tick data:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tick_vault</span> <span class=kn>import</span> <span class=n>download_range</span><span class=p>,</span> <span class=n>read_tick_data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Download a month of gold tick data</span>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>download_range</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Read it back as a pandas DataFrame</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>read_tick_data</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>head</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=c1># time                     ask        bid      ask_volume  bid_volume</span>
</span></span><span class=line><span class=cl><span class=c1># 2024-01-01 00:00:01.234  2062.450  2062.430  1500000    2300000</span>
</span></span></code></pre></div><h3 id=speed-it-up-with-proxies>Speed It Up with Proxies<a hidden class=anchor aria-hidden=true href=#speed-it-up-with-proxies>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Use multiple proxies to distribute load and avoid rate limits</span>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>download_range</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;EURUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2000</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=c1># Or datetime.now() for the same effect</span>
</span></span><span class=line><span class=cl>    <span class=n>proxies</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;http://proxy1.example.com:8080&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;http://proxy2.example.com:8080&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;http://proxy3.example.com:8080&#39;</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=configuration>Configuration<a hidden class=anchor aria-hidden=true href=#configuration>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tick_vault</span> <span class=kn>import</span> <span class=n>reload_config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Customize settings</span>
</span></span><span class=line><span class=cl><span class=n>reload_config</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>base_directory</span><span class=o>=</span><span class=s1>&#39;./my_tick_data&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>worker_per_proxy</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span>                <span class=c1># More workers per proxy</span>
</span></span><span class=line><span class=cl>    <span class=n>fetch_max_retry_attempts</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span>         <span class=c1># More retries for flaky connections</span>
</span></span><span class=line><span class=cl>    <span class=n>metadata_update_batch_size</span><span class=o>=</span><span class=mi>200</span>      <span class=c1># Larger batches for efficiency</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><p>Or use environment variables:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>export</span> <span class=nv>TICK_VAULT_BASE_DIRECTORY</span><span class=o>=</span>/data/ticks
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>TICK_VAULT_WORKER_PER_PROXY</span><span class=o>=</span><span class=m>15</span>
</span></span><span class=line><span class=cl><span class=nb>export</span> <span class=nv>TICK_VAULT_CONSOLE_LOG_LEVEL</span><span class=o>=</span>INFO
</span></span></code></pre></div><blockquote><p>Full configuration reference in the <a href=https://github.com/keyhankamyar/TickVault>repo</a></p></blockquote><h3 id=resuming-and-incremental-updates>Resuming and Incremental Updates<a hidden class=anchor aria-hidden=true href=#resuming-and-incremental-updates>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Download interrupted? Just run it again</span>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>download_range</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># TickVault checks metadata, skips completed chunks, resumes where it left off</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Update with recent data</span>
</span></span><span class=line><span class=cl><span class=k>await</span> <span class=n>download_range</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Only fetches the new chunks, leaves historical data untouched</span>
</span></span></code></pre></div><h3 id=data-validation>Data Validation<a hidden class=anchor aria-hidden=true href=#data-validation>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Strict mode (default): ensures the exact provided range is present</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>read_tick_data</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>strict</span><span class=o>=</span><span class=kc>True</span>  <span class=c1># Raises error if any hours are missing</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Non-strict mode: clips to available data range, still raises error if there are gaps between</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>read_tick_data</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2020</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>  <span class=c1># May be before first available</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2030</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>    <span class=c1># May be after last available</span>
</span></span><span class=line><span class=cl>    <span class=n>strict</span><span class=o>=</span><span class=kc>False</span>  <span class=c1># Automatically adjusts to what exists</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><h3 id=working-with-metadata>Working with Metadata<a hidden class=anchor aria-hidden=true href=#working-with-metadata>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tick_vault.metadata</span> <span class=kn>import</span> <span class=n>MetadataDB</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>MetadataDB</span><span class=p>()</span> <span class=k>as</span> <span class=n>db</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Check what&#39;s available</span>
</span></span><span class=line><span class=cl>    <span class=n>first</span> <span class=o>=</span> <span class=n>db</span><span class=o>.</span><span class=n>first_chunk</span><span class=p>(</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>last</span> <span class=o>=</span> <span class=n>db</span><span class=o>.</span><span class=n>last_chunk</span><span class=p>(</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Data range: </span><span class=si>{</span><span class=n>first</span><span class=o>.</span><span class=n>time</span><span class=si>}</span><span class=s2> to </span><span class=si>{</span><span class=n>last</span><span class=o>.</span><span class=n>time</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Find what needs downloading</span>
</span></span><span class=line><span class=cl>    <span class=n>pending</span> <span class=o>=</span> <span class=n>db</span><span class=o>.</span><span class=n>find_not_attempted_chunks</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;EURUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>pending</span><span class=p>)</span><span class=si>}</span><span class=s2> chunks remaining&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Verify continuity</span>
</span></span><span class=line><span class=cl>    <span class=n>db</span><span class=o>.</span><span class=n>check_for_gaps</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;XAUUSD&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>  <span class=c1># Raises RuntimeError if gaps exist</span>
</span></span></code></pre></div><h3 id=custom-price-scales>Custom Price Scales<a hidden class=anchor aria-hidden=true href=#custom-price-scales>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># For symbols not in the built-in registry</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>read_tick_data</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>symbol</span><span class=o>=</span><span class=s1>&#39;CUSTOM_PAIR&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>end</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2024</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>pipet_scale</span><span class=o>=</span><span class=mf>0.01</span>  <span class=c1># Specify your own scaling factor</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></div><hr><h2 id=-whats-next>ğŸ§­ What&rsquo;s Next<a hidden class=anchor aria-hidden=true href=#-whats-next>#</a></h2><p>TickVault works well for my needs, but there&rsquo;s room to grow. Here&rsquo;s the roadmap:</p><h3 id=download-pipeline>Download Pipeline<a hidden class=anchor aria-hidden=true href=#download-pipeline>#</a></h3><ul><li>Dynamic worker auto-balancing with throughput monitoring</li><li>Adaptive scaling: gradually increase workers until throughput plateaus, then back off</li><li>Async stop events for cleaner worker termination (currently uses sentinel values)</li></ul><h3 id=reading-pipeline>Reading Pipeline<a hidden class=anchor aria-hidden=true href=#reading-pipeline>#</a></h3><ul><li>Multi-threading and multi-processing support for parallel decoding (helpful for rapid SSDs)</li><li>Streaming decode-to-SQLite pipeline for memory-efficient querying</li><li>HDF5 storage backend option for large datasets</li></ul><h3 id=developer-experience>Developer Experience<a hidden class=anchor aria-hidden=true href=#developer-experience>#</a></h3><ul><li>CLI interface for common operations (<code>tickvault download XAUUSD --start 2024-01-01</code>)</li><li>Comprehensive pytest test suite</li></ul><h3 id=general-improvements>General Improvements<a hidden class=anchor aria-hidden=true href=#general-improvements>#</a></h3><ul><li>Reorganized module structure as the codebase grows</li><li>Unified <code>download_and_read()</code> convenience function</li><li>More symbols added to the pipet scale registry</li></ul><blockquote><p><strong>The core is stable.</strong> These are refinements, not fundamental changes. If you have ideas or want to contribute, issues and PRs are welcome.</p></blockquote><hr><h2 id=-try-it-out>â–¶ï¸ Try It Out<a hidden class=anchor aria-hidden=true href=#-try-it-out>#</a></h2><p>TickVault is <a href=https://github.com/keyhankamyar/TickVault>open source on GitHub</a>. If you&rsquo;ve fought with financial data pipelines, give it a shot:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install tick-vault
</span></span></code></pre></div><hr><p><strong>Star the repo</strong> if it solves a problem for you â€” Stars help other researchers discover tools that actually work.</p><p><strong>Open an issue or PR</strong> if you find bugs or have ideas â€” The codebase is minimal and clean enough to understand in an afternoon.</p><p><strong>Follow along</strong> for more posts on the RL research this was built for, performance optimizations, and lessons from building production data pipelines.</p><hr><h2 id=-final-thoughts>ğŸ’­ Final Thoughts<a hidden class=anchor aria-hidden=true href=#-final-thoughts>#</a></h2><p>I built TickVault because I needed tick data for reinforcement learning research and nothing else worked well enough.</p><p>If you&rsquo;re in the same boatâ€”tired of resampled data, broken tools, and expensive APIsâ€”<strong>this might help.</strong></p><hr><p><strong>Connect:</strong></p><p><a href=https://github.com/keyhankamyar/TickVault><img alt=GitHub loading=lazy src="https://img.shields.io/badge/GitHub-keyhankamyar%2FTickVault-181717?style=for-the-badge&logo=github"></a>
<a href=https://www.linkedin.com/in/keyhan-kamyar/><img alt=LinkedIn loading=lazy src="https://img.shields.io/badge/LinkedIn-keyhan--kamyar-0077B5?style=for-the-badge&logo=linkedin"></a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://keyhankamyar.github.io/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://keyhankamyar.github.io/tags/finance/>Finance</a></li><li><a href=https://keyhankamyar.github.io/tags/open-source/>Open-Source</a></li><li><a href=https://keyhankamyar.github.io/tags/python/>Python</a></li><li><a href=https://keyhankamyar.github.io/tags/data-engineering/>Data-Engineering</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on x" href="https://x.com/intent/tweet/?text=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To&amp;url=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f&amp;hashtags=machine-learning%2cfinance%2copen-source%2cpython%2cdata-engineering"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f&amp;title=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To&amp;summary=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To&amp;source=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f&title=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on whatsapp" href="https://api.whatsapp.com/send?text=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To%20-%20https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on telegram" href="https://telegram.me/share/url?text=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To&amp;url=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share I Downloaded 2,000,000+ Hours of Market Tick Data So You Don't Have To on ycombinator" href="https://news.ycombinator.com/submitlink?t=I%20Downloaded%202%2c000%2c000%2b%20Hours%20of%20Market%20Tick%20Data%20So%20You%20Don%27t%20Have%20To&u=https%3a%2f%2fkeyhankamyar.github.io%2fposts%2ftickvault-introduction%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>Â© 2025 Keyhan Kamyar</span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>